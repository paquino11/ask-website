from langchain_community.document_loaders import JSONLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv


def setup_and_run_rag_pipeline(json_path, question, model="gpt-4o-mini", chunk_size=1000, chunk_overlap=200):
    """
    Set up and run the RAG (Retrieval-Augmented Generation) pipeline.

    :param json_path: Path to the JSON file containing the documents.
    :param question: The input question to ask the model.
    :param model: The name of the OpenAI model to use.
    :param chunk_size: The size of each text chunk for splitting documents.
    :param chunk_overlap: The overlap between chunks when splitting.
    :return: The answer generated by the model.
    """

    load_dotenv()

    # initialize the llm
    llm = ChatOpenAI(model=model)

    # load the documents
    loader = JSONLoader(
        file_path=json_path,
        jq_schema='.[].content',
        text_content=False
    )

    docs = loader.load()

    # split documents into chunks
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    splits = text_splitter.split_documents(docs)

    # store: create embeddings and store them in a vector database
    vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())

    # Retrieval
    retriever = vectorstore.as_retriever()

    # Define the system prompt
    system_prompt = (
        "You are an assistant for question-answering tasks. "
        "Use the following pieces of retrieved context to answer "
        "the question. If you don't know the answer, say that you don't know."
        "\n\n"
        "{context}"
    )

    # Define the chat prompt template
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{input}"),
        ]
    )

    # Create chains
    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    # Run the pipeline with the input question
    results = rag_chain.invoke({"input": question})

    return results['answer']


# Example usage
# if __name__ == "__main__":
#     answer = setup_and_run_rag_pipeline(
#         json_path='./output.json',
#         question="What's an Agent?",
#     )
#     print(answer)
